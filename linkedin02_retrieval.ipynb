{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b205180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5386a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-29 14:41:23--  https://www.guttenberg.org/cache/epub/871/pg871.txt\n",
      "Resolving www.guttenberg.org (www.guttenberg.org)... 199.59.243.225\n",
      "Connecting to www.guttenberg.org (www.guttenberg.org)|199.59.243.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1086 (1,1K) [text/html]\n",
      "Saving to: ‘golden_hymns_of_epictetus.txt’\n",
      "\n",
      "golden_hymns_of_epi 100%[===================>]   1,06K  --.-KB/s    in 0,003s  \n",
      "\n",
      "2024-04-29 14:41:24 (407 KB/s) - ‘golden_hymns_of_epictetus.txt’ saved [1086/1086]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O golden_hymns_of_epictetus.txt https://www.guttenberg.org/cache/epub/871/pg871.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f17395",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_in = 'golden_hymns_of_epictetus.txt'\n",
    "filename_out = 'out_golden_hymns_of_epictetus.txt'\n",
    "start_saving = False\n",
    "stop_saving = False\n",
    "\n",
    "lines_to_save = []\n",
    "\n",
    "with open(filename_in, 'r') as file:\n",
    "    for line in file:\n",
    "        if 'Are these the only works of Providence within us?' in line:\n",
    "            start_saving = True\n",
    "        if '*** END OF THE PROJECT GUTTENBERG EBOOK THE GOLDEN' in line:\n",
    "            stop_saving = True\n",
    "            break\n",
    "        if start_saving and not stop_saving:\n",
    "            lines_to_save.append(line)\n",
    "\n",
    "# Scrittura delle righe in un nuovo file\n",
    "\n",
    "with open(filename_out, 'w') as file:\n",
    "    for line in lines_to_save:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3104abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il numero di words nel file è 26657\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "with open(filename_out, 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        word_count += len(words)\n",
    "        \n",
    "print(f'Il numero di words nel file è {word_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97220554",
   "metadata": {},
   "source": [
    "Utilizzo un TextLoader al fine di convertire un file di testo in un formato adatto all'utilizzo da parte di un llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a637f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(filename_out)\n",
    "golden_saying = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902d89bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(golden_saying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13443fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(golden_saying[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29074df3",
   "metadata": {},
   "source": [
    "Oltre a Textloader ci sono parecchi altri document loader in langchain.<br>\n",
    "Gli step per utilizzarli sono sempre:\n",
    "- scegliere un Document Loader adatto alle esigenza\n",
    "- creare una istanza \n",
    "- utilizzare il metodo load() per convertire i files in langchain documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9e674",
   "metadata": {},
   "source": [
    "Per dividere il testo in parti più ridotte e gestibili si utilizza il TEXT SPLITTER.<br>\n",
    "Il questo caso utilizziamo il RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab75d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d01a65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_splitter.split_documents(golden_saying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f79fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Are these the only works of Providence within us? What words suffice to\\npraise or set them forth? Had we but understanding, should we ever cease\\nhymning and blessing the Divine Power, both openly and in secret, and\\ntelling of His gracious gifts? Whether digging or ploughing or eating,\\nshould we not sing the hymn to God:‐‐\\nGreat is God, for that He hath given us such instruments to till\\nthe ground withal: Great is God, for that He hath given us hands and\\nthe power of swallowing and digesting; of unconsciously growing and\\nbreathing while we sleep!\\nThus should we ever have sung; yea and this, the grandest and divinest\\nhymn of all:‐‐\\nGreat is God, for that He hath given us a mind to apprehend these\\nthings, and duly to use them!\\nWhat then! seeing that most of you are blinded, should there not be some\\none to fill this place, and sing the hymn to God on behalf of all\\nmen? What else can I that am old and lame do but sing to God? Were I' metadata={'source': 'out_golden_hymns_of_epictetus.txt', 'start_index': 0}\n",
      "page_content='a nightingale, I should do after the manner of a nightingale. Were I\\na swan, I should do after the manner of a swan. But now, since I am a\\nreasonable being, I must sing to God: that is my work: I do it, nor will\\nI desert this my post, as long as it is granted me to hold it; and upon\\nyou too I call to join in this self‐same hymn.\\n08/03/2017 www.gutenberg.org/cache/epub/871/pg871.txt\\nhttp://www.gutenberg.org/cache/epub/871/pg871.txt 2/57\\nII\\nHow then do men act? As though one returning to his country who had\\nsojourned for the night in a fair inn, should be so captivated thereby\\nas to take up his abode there.\\n\"Friend, thou hast forgotten thine intention! This was not thy\\ndestination, but only lay on the way thither.\"\\n\"Nay, but it is a proper place.\"\\n\"And how many more of the sort there may be; only to pass through\\nupon thy way! Thy purpose was to return to thy country; to relieve thy\\nkinsmen\\'s fears for thee; thyself to discharge the duties of a citizen;' metadata={'source': 'out_golden_hymns_of_epictetus.txt', 'start_index': 942}\n"
     ]
    }
   ],
   "source": [
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d8f9b",
   "metadata": {},
   "source": [
    "Dopo aver suddiviso il testo iniziale in parti ridotte, è possibile procedere con la coversione dei testi in <B>EMBEDDINGS<B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4af30469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        api_key='545454545',\n",
    "        base_url='http://localhost:8000/v1',\n",
    "        chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04866635",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': \"[{'type': 'string_type', 'loc': ('body', 'input', 'str'), 'msg': 'Input should be a valid string', 'input': [[11787, 1521, 279, 1193, 4375, 315, 58941, 2949, 603, 30, 3639, 4339, 77256, 311, 198, 652, 4105, 477, 743, 1124, 13544, 30, 24805, 584, 719, 8830, 11, 1288, 584, 3596, 32616, 198, 71, 1631, 1251, 323, 40149, 279, 43361, 7572, 11, 2225, 30447, 323, 304, 6367, 11, 323, 198, 83, 6427, 315, 5414, 82426, 21258, 30, 13440, 42200, 477, 628, 1409, 287, 477, 12459, 345, 5562, 584, 539, 7936, 279, 67743, 77, 311, 4359, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 1778, 24198, 311, 12222, 198, 1820, 5015, 449, 278, 25, 8681, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 6206, 323, 198, 1820, 2410, 315, 91747, 323, 21552, 287, 26, 315, 26118, 71888, 7982, 323, 198, 21152, 44661, 1418, 584, 6212, 4999, 45600, 1288, 584, 3596, 617, 40439, 26, 379, 12791, 323, 420, 11, 279, 6800, 478, 323, 3512, 258, 478, 198, 71, 1631, 77, 315, 682, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 264, 4059, 311, 47291, 408, 1521, 198, 58174, 11, 323, 77903, 311, 1005, 1124, 4999, 3923, 1243, 0, 9298, 430, 1455, 315, 499, 527, 90216, 11, 1288, 1070, 539, 387, 1063, 198, 606, 311, 5266, 420, 2035, 11, 323, 7936, 279, 67743, 77, 311, 4359, 389, 17981, 315, 682, 198, 5794, 30, 3639, 775, 649, 358, 430, 1097, 2362, 323, 61983, 656, 719, 7936, 311, 4359, 30, 40070, 358]], 'url': 'https://errors.pydantic.dev/2.6/v/string_type'}, {'type': 'string_type', 'loc': ('body', 'input', 'list[str]', 0), 'msg': 'Input should be a valid string', 'input': [11787, 1521, 279, 1193, 4375, 315, 58941, 2949, 603, 30, 3639, 4339, 77256, 311, 198, 652, 4105, 477, 743, 1124, 13544, 30, 24805, 584, 719, 8830, 11, 1288, 584, 3596, 32616, 198, 71, 1631, 1251, 323, 40149, 279, 43361, 7572, 11, 2225, 30447, 323, 304, 6367, 11, 323, 198, 83, 6427, 315, 5414, 82426, 21258, 30, 13440, 42200, 477, 628, 1409, 287, 477, 12459, 345, 5562, 584, 539, 7936, 279, 67743, 77, 311, 4359, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 1778, 24198, 311, 12222, 198, 1820, 5015, 449, 278, 25, 8681, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 6206, 323, 198, 1820, 2410, 315, 91747, 323, 21552, 287, 26, 315, 26118, 71888, 7982, 323, 198, 21152, 44661, 1418, 584, 6212, 4999, 45600, 1288, 584, 3596, 617, 40439, 26, 379, 12791, 323, 420, 11, 279, 6800, 478, 323, 3512, 258, 478, 198, 71, 1631, 77, 315, 682, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 264, 4059, 311, 47291, 408, 1521, 198, 58174, 11, 323, 77903, 311, 1005, 1124, 4999, 3923, 1243, 0, 9298, 430, 1455, 315, 499, 527, 90216, 11, 1288, 1070, 539, 387, 1063, 198, 606, 311, 5266, 420, 2035, 11, 323, 7936, 279, 67743, 77, 311, 4359, 389, 17981, 315, 682, 198, 5794, 30, 3639, 775, 649, 358, 430, 1097, 2362, 323, 61983, 656, 719, 7936, 311, 4359, 30, 40070, 358], 'url': 'https://errors.pydantic.dev/2.6/v/string_type'}]\", 'type': 'internal_server_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m----> 3\u001b[0m vectorsore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_core/vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:930\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    911\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 930\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m    932\u001b[0m         texts,\n\u001b[1;32m    933\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    938\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:517\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:333\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    331\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 333\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    337\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:997\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    996\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:1045\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:997\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    996\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:1045\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain_env/lib/python3.12/site-packages/openai/_base_client.py:1012\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1011\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1016\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1020\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': \"[{'type': 'string_type', 'loc': ('body', 'input', 'str'), 'msg': 'Input should be a valid string', 'input': [[11787, 1521, 279, 1193, 4375, 315, 58941, 2949, 603, 30, 3639, 4339, 77256, 311, 198, 652, 4105, 477, 743, 1124, 13544, 30, 24805, 584, 719, 8830, 11, 1288, 584, 3596, 32616, 198, 71, 1631, 1251, 323, 40149, 279, 43361, 7572, 11, 2225, 30447, 323, 304, 6367, 11, 323, 198, 83, 6427, 315, 5414, 82426, 21258, 30, 13440, 42200, 477, 628, 1409, 287, 477, 12459, 345, 5562, 584, 539, 7936, 279, 67743, 77, 311, 4359, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 1778, 24198, 311, 12222, 198, 1820, 5015, 449, 278, 25, 8681, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 6206, 323, 198, 1820, 2410, 315, 91747, 323, 21552, 287, 26, 315, 26118, 71888, 7982, 323, 198, 21152, 44661, 1418, 584, 6212, 4999, 45600, 1288, 584, 3596, 617, 40439, 26, 379, 12791, 323, 420, 11, 279, 6800, 478, 323, 3512, 258, 478, 198, 71, 1631, 77, 315, 682, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 264, 4059, 311, 47291, 408, 1521, 198, 58174, 11, 323, 77903, 311, 1005, 1124, 4999, 3923, 1243, 0, 9298, 430, 1455, 315, 499, 527, 90216, 11, 1288, 1070, 539, 387, 1063, 198, 606, 311, 5266, 420, 2035, 11, 323, 7936, 279, 67743, 77, 311, 4359, 389, 17981, 315, 682, 198, 5794, 30, 3639, 775, 649, 358, 430, 1097, 2362, 323, 61983, 656, 719, 7936, 311, 4359, 30, 40070, 358]], 'url': 'https://errors.pydantic.dev/2.6/v/string_type'}, {'type': 'string_type', 'loc': ('body', 'input', 'list[str]', 0), 'msg': 'Input should be a valid string', 'input': [11787, 1521, 279, 1193, 4375, 315, 58941, 2949, 603, 30, 3639, 4339, 77256, 311, 198, 652, 4105, 477, 743, 1124, 13544, 30, 24805, 584, 719, 8830, 11, 1288, 584, 3596, 32616, 198, 71, 1631, 1251, 323, 40149, 279, 43361, 7572, 11, 2225, 30447, 323, 304, 6367, 11, 323, 198, 83, 6427, 315, 5414, 82426, 21258, 30, 13440, 42200, 477, 628, 1409, 287, 477, 12459, 345, 5562, 584, 539, 7936, 279, 67743, 77, 311, 4359, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 1778, 24198, 311, 12222, 198, 1820, 5015, 449, 278, 25, 8681, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 6206, 323, 198, 1820, 2410, 315, 91747, 323, 21552, 287, 26, 315, 26118, 71888, 7982, 323, 198, 21152, 44661, 1418, 584, 6212, 4999, 45600, 1288, 584, 3596, 617, 40439, 26, 379, 12791, 323, 420, 11, 279, 6800, 478, 323, 3512, 258, 478, 198, 71, 1631, 77, 315, 682, 25, 31021, 31021, 198, 22111, 374, 4359, 11, 369, 430, 1283, 52677, 2728, 603, 264, 4059, 311, 47291, 408, 1521, 198, 58174, 11, 323, 77903, 311, 1005, 1124, 4999, 3923, 1243, 0, 9298, 430, 1455, 315, 499, 527, 90216, 11, 1288, 1070, 539, 387, 1063, 198, 606, 311, 5266, 420, 2035, 11, 323, 7936, 279, 67743, 77, 311, 4359, 389, 17981, 315, 682, 198, 5794, 30, 3639, 775, 649, 358, 430, 1097, 2362, 323, 61983, 656, 719, 7936, 311, 4359, 30, 40070, 358], 'url': 'https://errors.pydantic.dev/2.6/v/string_type'}]\", 'type': 'internal_server_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorsore = FAISS.from_documents(documents=text, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53f9b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: tqdm in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from sentence_transformers) (4.66.2)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Using cached torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: numpy in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from sentence_transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Using cached scipy-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from sentence_transformers) (0.22.2)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Using cached pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
      "Collecting sympy (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.4.16)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Using cached safetensors-0.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "Using cached torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m685.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:08\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m759.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m879.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m444.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m767.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m486.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m479.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m640.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m554.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence_transformers\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.3.0 jinja2-3.1.3 joblib-1.4.0 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.3 scikit-learn-1.4.2 scipy-1.13.0 sentence_transformers-2.7.0 sympy-1.12 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.0 transformers-4.40.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bff4088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5dde786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34bc989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /home/michele/anaconda3/envs/langchain_env/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Downloading faiss_cpu-1.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m987.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ab13a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=text, embedding=embeddings_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8788c727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Something of the same sort is true also of diseases of the mind. Behind,\\nthere remains a legacy of traces and blisters: and unless these are\\neffectually erased, subsequent blows on the same spot will produce\\nno longer mere blisters, but sores. If you do not wish to be prone\\nto anger, do not feed the habit; give it nothing which may tend its\\nincrease. At first, keep quiet and count the days when you were not\\nangry: \"I used to be angry every day, then every other day: next every\\ntwo, next every three days!\" and if you succeed in passing thirty days,\\nsacrifice to the Gods in thanksgiving.\\nLXXVI\\nHow then may this be attained?‐‐Resolve, now if never before, to approve\\nthyself to thyself; resolve to show thyself fair in God\\'s sight; long to\\nbe pure with thine own pure self and God!\\nLXXVII\\nThat is the true athlete, that trains himself to resist such outward\\nimpressions as these.\\n\"Stay, wretched man! suffer not thyself to be carried away!\" Great is', metadata={'source': 'out_golden_hymns_of_epictetus.txt', 'start_index': 52993}),\n",
       " Document(page_content='to study to its proper end, what else is this than a life that flows on\\ntranquil and serene? And if thy reading secures thee not serenity, what\\nprofits it?‐‐\"Nay, but it doth secure it,\" quoth he, \"and that is why I\\nrepine at being deprived of it.\"‐‐And what serenity is this that lies at\\nthe mercy of every passer‐by? I say not at the mercy of the Emperor or\\nEmperor\\'s favorite, but such as trembles at a raven\\'s croak and piper\\'s\\ndin, a fever\\'s touch or a thousand things of like sort! Whereas the\\nlife serene has no more certain mark than this, that it ever moves with\\nconstant unimpeded flow.\\nCXLVI\\nIf thou hast put malice and evil speaking from thee, altogether, or\\nin some degree: if thou hast put away from thee rashness, foulness of\\ntongue, intemperance, sluggishness: if thou art not moved by what once\\nmoved thee, or in like manner as thou once wert moved‐‐then thou mayest\\ncelebrate a daily festival, to‐day because thou hast done well in this', metadata={'source': 'out_golden_hymns_of_epictetus.txt', 'start_index': 102938}),\n",
       " Document(page_content='be free from passion and from perturbation; as one who grudges no pains\\nin the pursuit of piety and philosophy, what I desire is to know my duty\\nto the Gods, my duty to my parents, to my brothers, to my country, to\\nstrangers.\"\\n\"Enter then on the second part of the subject; it is thine also.\"\\n\"But I have already mastered the second part; only I wished to stand\\nfirm and unshaken‐‐as firm when asleep as when awake, as firm when\\nelated with wine as in despondency and dejection.\"\\n\"Friend, you are verily a God! you cherish great designs.\"\\nLXXIV\\n\"The question at stake,\" said Epictetus, \"is no common one; it is\\n08/03/2017 www.gutenberg.org/cache/epub/871/pg871.txt\\nhttp://www.gutenberg.org/cache/epub/871/pg871.txt 21/57\\nthis:‐‐Are we in our senses, or are we not?\"\\nLXXV\\nIf you have given way to anger, be sure that over and above the evil\\ninvolved therein, you have strengthened the habit, and added fuel to\\nthe fire. If overcome by a temptation of the flesh, do not reckon it', metadata={'source': 'out_golden_hymns_of_epictetus.txt', 'start_index': 51047}),\n",
       " Document(page_content='vigilance. He paid however this price for the lamp, that in exchange\\nfor it he consented to become a thief: in exchange for it, to become\\nfaithless.\\nXIII\\nBut God hath introduced Man to be a spectator of Himself and of His\\nworks; and not a spectator only, but also an interpreter of them.\\nWherefore it is a shame for man to begin and to leave off where the\\nbrutes do. Rather he should begin there, and leave off where Nature\\nleaves off in us: and that is at contemplation, and understanding, and a\\nmanner of life that is in harmony with herself.\\nSee then that ye die not without being spectators of these things.\\nXIV\\nYou journey to Olympia to see the work of Phidias; and each of you holds\\nit a misfortune not to have beheld these things before you die. Whereas\\nwhen there is no need even to take a journey, but you are on the spot,\\nwith the works before you, have you no care to contemplate and study\\nthese?\\nWill you not then perceive either who you are or unto what end you were', metadata={'source': 'out_golden_hymns_of_epictetus.txt', 'start_index': 6771})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How can I practice mindfulness if I am always so busy and distracted?\"\n",
    "\n",
    "vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cf895",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cc47cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Grief, fear, anvy, and desire stem from a lack of understanding or knowledge about the world around us. They are caused by our inability to accept or understand certain situations or events that we encounter in life. These emotions can be overcome by learning more about ourselves and the world around us, and by developing a greater sense of understanding and acceptance. By doing so, we can learn to deal with these emotions in a more constructive way and find peace and happiness in our lives.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "If you don't know the answer, just say 'I don't know', don't try to make up an answer.\n",
    "\n",
    "Use three sentence maximump and keep the answer as concise as possible.\n",
    "\n",
    "Use the active voice and speak directly to the reader using a concise language.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpfull Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "openai_llm = ChatOpenAI(\n",
    "        api_key='545454545',\n",
    "        base_url='http://localhost:8000/v1',\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    openai_llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\" : QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "query = \"What do grief, fear, anvy and desire stem from?\"\n",
    "\n",
    "result = qa_chain.invoke({\"query\" : query})\n",
    "\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf1544",
   "metadata": {},
   "source": [
    "### Utilizzo di LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23da4731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Grief, fear, anvy, and desire stem from a lack of understanding or acceptance of reality. They are caused by a disconnect between what we want and what we have, or between what we believe is right and what we experience in life. They are also caused by a lack of control over our circumstances or a lack of understanding of how our actions affect our lives. To overcome these negative emotions, we need to develop a deeper understanding of ourselves and the world around us, and learn to accept what we cannot change. We also need to develop a sense of purpose and meaning in life, and learn to focus on what we can control rather than what we cannot. By doing so, we can find peace and happiness in life.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel({\"context\" : vectorstore.as_retriever(), \"question\" : RunnablePassthrough()})\n",
    "\n",
    "chain = setup_and_retrieval | QA_CHAIN_PROMPT | openai_llm | output_parser\n",
    "\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bca404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_env]",
   "language": "python",
   "name": "conda-env-langchain_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
